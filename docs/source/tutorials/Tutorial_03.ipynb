{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "48e48d7f-053e-4a1b-8f95-f2c083c949cd",
      "cell_type": "markdown",
      "source": "# Tutorial 03: TENAX Hindcast evaluation\nThis tutorial guides you through the process of implementing the scaling of TENAX-based changes in mean temperature (ðœ‡\nÎ¼) and the standard deviation of temperature (Ïƒ) during precipitation events.\n\nAlthough this method can also be used to project changes in extreme sub-hourly precipitation under a future warmer climate, it relies solely on climate model projections of temperatures during wet days and anticipated changes in precipitation frequency.\"",
      "metadata": {}
    },
    {
      "id": "24266dc5-298b-47a7-8884-6b391dd78400",
      "cell_type": "markdown",
      "source": "Import all libraries needed for running TENAX and SMEV",
      "metadata": {}
    },
    {
      "id": "9fb356f4-980b-473d-bdc7-1d4b0a3d8d3a",
      "cell_type": "code",
      "source": "from importlib.resources import files\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2\n# Import pyTENAX\nfrom pyTENAX import tenax, plotting",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d075c82d-500c-46a9-ab7c-bdd65283d44b",
      "cell_type": "markdown",
      "source": "Let's initiate TENAX class and SMEV class with given setup.",
      "metadata": {}
    },
    {
      "id": "2f471a9b-98b3-450e-b326-60ad6117cd08",
      "cell_type": "code",
      "source": "# Initiate TENAX class with customized setup\nS = tenax.TENAX(\n    return_period=[\n        2,\n        5,\n        10,\n        20,\n        50,\n        100,\n        200,\n    ],\n    durations=[10, 60, 180, 360, 720, 1440], #durations are in minutes and they refer to depth of rainfall within given duration\n    time_resolution=5,  # time resolution in minutes\n    left_censoring=[0, 0.90], # left censoring threshold \n    alpha=0.05, #dependence of shape on T depends on statistical significance at the alpha-level.\n    min_rain = 0.1, #minimum rainfall depth threshold\n)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1641d7d1-9551-4417-b039-c9b1d10a5660",
      "cell_type": "markdown",
      "source": "Once again we start with same data.",
      "metadata": {}
    },
    {
      "id": "b43a02dd-66fc-4007-ba4d-f16f10247962",
      "cell_type": "code",
      "source": "# Load precipitation data\n# Create input path file for the test file\nfile_path_input = files('pyTENAX.res').joinpath('prec_data_Aadorf.parquet')\n# Load data from csv file\ndata = pd.read_parquet(file_path_input)\n# Convert 'prec_time' column to datetime, if it's not already\ndata[\"prec_time\"] = pd.to_datetime(data[\"prec_time\"])\n# Set 'prec_time' as the index\ndata.set_index(\"prec_time\", inplace=True)\nname_col = \"prec_values\"  # name of column containing data to extract\n\n# load temperature data\nfile_path_temperature = files('pyTENAX.res').joinpath('temp_data_Aadorf.parquet')\nt_data = pd.read_parquet(file_path_temperature)\n# Convert 'temp_time' column to datetime if it's not already in datetime format\nt_data[\"temp_time\"] = pd.to_datetime(t_data[\"temp_time\"])\n# Set 'temp_time' as the index\nt_data.set_index(\"temp_time\", inplace=True)\ntemp_name_col = \"temp_values\"",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d716d3c5-4bff-4465-bd59-a97e9d9c242a",
      "cell_type": "markdown",
      "source": "## Repeat the preprocessing to get ordinary events values and corresponding temperature.\nWe once again focus only on 10-minuts rainfall depth.",
      "metadata": {}
    },
    {
      "id": "f4240b96-48dd-4767-ac21-644fd9e82fe3",
      "cell_type": "code",
      "source": "data = S.remove_incomplete_years(data, name_col)\n\n# get data from pandas to numpy array\ndf_arr = np.array(data[name_col])\ndf_dates = np.array(data.index)\ndf_arr_t_data = np.array(t_data[temp_name_col])\ndf_dates_t_data = np.array(t_data.index)\n\n# extract indexes of ordinary events\n# these are time-wise indexes =>returns list of np arrays with np.timeindex\nidx_ordinary = S.get_ordinary_events(data=df_arr, \n                                     dates=df_dates, \n                                     name_col=name_col,\n                                     check_gaps=False)\n\n# get ordinary events by removing too short events\n# returns boolean array, dates of OE in TO, FROM format, and count of OE in each years\narr_vals, arr_dates, n_ordinary_per_year = S.remove_short(idx_ordinary)\n\n# assign ordinary events values by given durations, values are in depth per duration, NOT in intensity mm/h\ndict_ordinary, dict_AMS = S.get_ordinary_events_values(data=df_arr, \n                                                       dates=df_dates, \n                                                       arr_dates_oe=arr_dates)\n\ndict_ordinary, _, n_ordinary_per_year = S.associate_vars(dict_ordinary, \n                                                         df_arr_t_data, \n                                                         df_dates_t_data)\n\n# Your data (P, T arrays) and threshold thr=3.8\nP = dict_ordinary[\"10\"][\"ordinary\"].to_numpy()  # Replace with your actual data\nT = dict_ordinary[\"10\"][\"T\"].to_numpy()  # Replace with your actual data\nblocks_id = dict_ordinary[\"10\"][\"year\"].to_numpy()  # Replace with your actual data\n# Number of threshold\nthr = dict_ordinary[\"10\"][\"ordinary\"].quantile(S.left_censoring[1])\n# Exctract annual maximas\nAMS = dict_AMS[\"10\"]  # yet the annual maxima\n\n# For plotting \n# we must create range of temperature\neT = np.arange(\n    np.min(T)-4, np.max(T) + 4, 1\n)  # define T values to calculate distributions. +4 to go beyond graph end\n\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bdf70a0b-a461-4201-a6df-8378918fd4ac",
      "cell_type": "markdown",
      "source": "## Hindcast evaluation\nWe evaluated the ability of the TENAX model to project precipitation return levels under increased temperatures through a hindcast, by splitting the 38-year record of the climate station into two 19-year periods.",
      "metadata": {}
    },
    {
      "id": "84b1fa4f-abd1-4655-8026-f4625d9e8b11",
      "cell_type": "code",
      "source": "yrs = dict_ordinary[\"10\"][\"oe_time\"].dt.year\nyrs_unique = np.unique(yrs)\nmidway = yrs_unique[\n    int(np.ceil(np.size(yrs_unique) / 2)) - 1\n]  # -1 to adjust indexing because this returns a sort of length\n\n# DEFINE FIRST PERIOD\nP1 = P[yrs <= midway]\nT1 = T[yrs <= midway]\nAMS1 = AMS[AMS[\"year\"] <= midway]\nn_ordinary_per_year1 = n_ordinary_per_year[n_ordinary_per_year.index <= midway]\nn1 = n_ordinary_per_year1.sum() / len(n_ordinary_per_year1)\n\n# DEFINE SECOND PERIOD\nP2 = P[yrs > midway]\nT2 = T[yrs > midway]\nAMS2 = AMS[AMS[\"year\"] > midway]\nn_ordinary_per_year2 = n_ordinary_per_year[n_ordinary_per_year.index > midway]\nn2 = n_ordinary_per_year2.sum() / len(n_ordinary_per_year2)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "119523a5-74da-49b3-b96b-0b29f66a163f",
      "cell_type": "markdown",
      "source": "## Comparing Temperature models in two periods (1981-1999 & 2000-2018)",
      "metadata": {}
    },
    {
      "id": "c65dd6b8-a3da-4c35-8c87-fcbabcb1765a",
      "cell_type": "code",
      "source": "g_phat1 = S.temperature_model(T1) #returns mu and sigma\ng_phat2 = S.temperature_model(T2) #returns mu and sigma\n\n_, _ = plotting.TNX_FIG_temp_model(\n    T=T1,\n    g_phat=g_phat1,\n    beta=4,\n    eT=eT,\n    obscol=\"b\",\n    valcol=\"b\",\n    obslabel=None,\n    vallabel=\"Temperature model \" + str(yrs_unique[0]) + \"-\" + str(midway),\n)\n_, _ = plotting.TNX_FIG_temp_model(\n    T=T2,\n    g_phat=g_phat2,\n    beta=4,\n    eT=eT,\n    obscol=\"r\",\n    valcol=\"r\",\n    obslabel=None,\n    vallabel=\"Temperature model \" + str(midway + 1) + \"-\" + str(yrs_unique[-1]),\n)  # model based on temp ave and std changes",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e93bdb4b-2bd4-42bf-9fab-5886f59aa5cc",
      "cell_type": "markdown",
      "source": "### Predicted Temperature Model: Based on Changes in Î¼ (Mean) and Ïƒ (Standard Deviation)\n\nIncreases in the mean temperature (Î¼) and/or the standard deviation (Ïƒ) imply a higher probability of precipitation events occurring at higher temperatures.\n\n- `mu_delta` represents the change in mean temperature during precipitation events.\n- `sigma_factor` represents the scaling factor applied to the standard deviation of temperature during precipitation events.\n\nThe predicted temperature model is computed by:\n\n- Adding `mu_delta` to the original mean (Î¼), and  \n- Multiplying the original standard deviation (Ïƒ) by `sigma_factor`.\n\n#### Mathematical Formulation\n\nLet:\n\n- Î¼ = original mean temperature of first model (1981-1999)\n- Ïƒ = original standard deviation of first model (1981-1999) \n- Î¼' = predicted mean temperature  \n- Ïƒ' = predicted standard deviation  \n- Î”Î¼ = `mu_delta`  \n- Ïƒ_factor = `sigma_factor`\n\nThen:\n\n$$\n\\mu' = \\mu + \\Delta\\mu\n$$\n\n$$\n\\sigma' = \\sigma \\times \\sigma_{\\text{factor}}\n$$",
      "metadata": {}
    },
    {
      "id": "b32c925c-5a42-424e-94f9-29a50c9c0494",
      "cell_type": "code",
      "source": "mu_delta = np.mean(T2) - np.mean(T1)\nsigma_factor = np.std(T2) / np.std(T1)\nprint(f\"Mean temperature has changed by {mu_delta}\")\nprint(f\"Standart deviation has changed by factor of {sigma_factor}\")\n\n# Create a predicted temperature model\ng_phat2_predict = [g_phat1[0] + mu_delta, g_phat1[1] * sigma_factor]\n\n# Compare with Temperatude model of second period we have created before \nprint(f\"Temperatude model of second period {g_phat2}\")\nprint(f\"Predicted Temperatude model {g_phat2_predict}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ef46f58c-ec5a-4546-8092-4c80139d464c",
      "cell_type": "markdown",
      "source": "## Create TENAX Magnitude models of two periods (1981-1999 & 2000-2018)\nA magnitude model $W(x; T)$ was fitted independently for each time period. To assess the similarity of the models, a likelihood ratio test was applied.\n\nAccording to the general theory of the likelihood ratio test, under the null hypothesis $H_0$, we compute the likelihood function $L(\\theta)$, where $\\theta \\in \\Theta$ and $\\Theta$ is the parameter space.\n\nThe test statistic is defined as:\n\n$$\n-2 \\ln \\left( \\frac{\\sup_{\\theta \\in H_0} L(\\theta)}{\\sup_{\\theta \\in \\Theta} L(\\theta)} \\right)\n$$\n\nThis statistic follows a chi-squared distribution under certain regularity conditions and can be used to determine whether the models fitted for different periods are significantly different.\n\n",
      "metadata": {}
    },
    {
      "id": "b1c5a66a-88e1-4121-8bff-3c33a53e5f4e",
      "cell_type": "code",
      "source": "# Sampling intervals for the Montecarlo based on original temperature T\nTs = np.arange(\n    np.min(T) - S.temp_delta, np.max(T) + S.temp_delta, S.temp_res_monte_carlo\n)\n\n# Maginitude model of original data that containst both periods (1981-2018)\nF_phat, loglik, _, _ = S.magnitude_model(P, T, thr)\n\n# Maginitude model of first period (1981-1999)\nF_phat1, loglik1, _, _ = S.magnitude_model(P1, T1, thr)\nRL1, _, _ = S.model_inversion(F_phat1, g_phat1, n1, Ts)\n\n# Maginitude model of second period  (2000-2018)\nF_phat2, loglik2, _, _ = S.magnitude_model(P2, T2, thr)\nRL2, _, _ = S.model_inversion(F_phat2, g_phat2, n2, Ts)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f1478024-0911-498f-b8ee-20128e10e702",
      "cell_type": "code",
      "source": "if F_phat[1] == 0:  # check if b parameter is 0 (shape=shape_0*b\n    dof = 3\n    alpha1 = 1  # b parameter is not significantly different from 0; 3 degrees of freedom for the LR test\nelse:\n    dof = 4\n    alpha1 = 0  # b parameter is significantly different from 0; 4 degrees of freedom for the LR test",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3015f327-f0f6-4686-a076-aeb7cd62c086",
      "cell_type": "code",
      "source": "# check magnitude model the same in both periods\nlambda_LR = -2 * (loglik - (loglik1 + loglik2))\npval = chi2.sf(lambda_LR, dof)\nif pval > S.alpha:\n    print(f\"p={pval}. Magnitude models not  different at {S.alpha*100}% significance.\")\nelse:\n    print(f\"p={pval}. Magnitude models are different at {S.alpha*100}% significance.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "782db30b-7b8c-4ea5-92e5-84f4ea3d1887",
      "cell_type": "markdown",
      "source": "## Estimating return levels based on projected change in temperature model and by using the magnitude model of first period. \n\nWe compare these return levels to annual maxima of second period and \nNote: Here we do not apply scaling of n (the average number of events per year). \nAlthough, this can be simply done by calculating ratio difference of events during two periods and multipying n1 by this factor.",
      "metadata": {}
    },
    {
      "id": "bcc7f1d3-bfba-440b-8548-19065c26ab65",
      "cell_type": "code",
      "source": "RL2_predict, _, _ = S.model_inversion(F_phat1, g_phat2_predict, n1, Ts)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "afbc98cf-4c99-4728-8c35-d26aeb064b04",
      "cell_type": "code",
      "source": "# Plot the results\nplotting.TNX_FIG_valid(\n    AMS1,\n    S.return_period,\n    RL1,\n    TENAXcol=\"b\",\n    obscol_shape=\"b+\",\n    TENAXlabel=\"The TENAX model \" + str(yrs_unique[0]) + \"-\" + str(midway),\n    obslabel=\"Observed annual maxima \" + str(yrs_unique[0]) + \"-\" + str(midway),\n)\nplotting.TNX_FIG_valid(\n    AMS2,\n    S.return_period,\n    RL2_predict,\n    TENAXcol=\"r\",\n    obscol_shape=\"r+\",\n    TENAXlabel=\"The predicted TENAX model \"\n    + str(midway + 1)\n    + \"-\"\n    + str(yrs_unique[-1]),\n    obslabel=\"Observed annual maxima \" + str(midway + 1) + \"-\" + str(yrs_unique[-1]),\n)\nplt.xticks(S.return_period)\nplt.gca().set_xticks(S.return_period)  # This sets the actual tick marks on log scale\nplt.gca().get_xaxis().set_major_formatter(plt.ScalarFormatter())  # Optional: shows ticks as plain numbers\nplt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.2))\nplt.grid(True, which='both', axis='both', linestyle='--', color='lightgray', alpha=0.7)\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}